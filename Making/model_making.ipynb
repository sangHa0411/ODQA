{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import collections"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_path=\"/opt/ml/project/odqa/data/\"\n",
    "context_path= \"wikipedia_documents.json\"\n",
    "with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "wiki_data = []\n",
    "\n",
    "for key in wiki.keys() :\n",
    "    data = wiki[key]\n",
    "    doc_id = data['document_id']\n",
    "    text = data['text']\n",
    "    wiki_data.append(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Callable, NoReturn, NewType, Any\n",
    "import dataclasses\n",
    "from datasets import load_metric, load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForQuestionAnswering, AutoTokenizer, BertTokenizer, BertTokenizerFast\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "sys.path.append('../')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from arguments import (\n",
    "    ModelArguments,\n",
    "    DataTrainingArguments,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model_args = ModelArguments\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name \n",
    "    if model_args.config_name is not None\n",
    "    else model_args.model_name_or_path,\n",
    ")\n",
    "print(config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data_args = DataTrainingArguments"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "bert_model = AutoModel.from_pretrained('klue/roberta-large')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name\n",
    "    if model_args.tokenizer_name is not None\n",
    "    else model_args.model_name_or_path,\n",
    "    use_fast=True,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print('Tokenizer Size : %d' %len(tokenizer))\n",
    "print(tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizer Size : 32000\n",
      "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tokenizer_output = tokenizer(wiki_data[0], return_tensors='pt', padding='max_length',max_length=384)\n",
    "tokenizer_output = {key:tokenizer_output[key] for key in tokenizer_output }\n",
    "\n",
    "model_input = {'input_ids' : tokenizer_output['input_ids'], \n",
    "    'attention_mask' : tokenizer_output['attention_mask'], \n",
    "    'token_type_ids' : tokenizer_output['token_type_ids']}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model_output = bert_model(**model_input)[0]\n",
    "print('Model output shape : {}'.format(model_output.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model output shape : torch.Size([1, 384, 1024])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN Head"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class ConvLayer(nn.Module) :\n",
    "    def __init__(self, seq_size, feature_size, intermediate_size) :\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(nn.Conv1d(seq_size, intermediate_size, 5, padding=2),\n",
    "            nn.Conv1d(intermediate_size, seq_size, 1),\n",
    "            nn.ReLU())\n",
    "        self.layer_norm = nn.LayerNorm(feature_size, eps=1e-6)\n",
    "            \n",
    "    def forward(self, x) :\n",
    "        y = x + self.conv_layer(x)\n",
    "        y = self.layer_norm(y)\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class ConvNet(nn.Module) :\n",
    "    def __init__(self, layer_size, seq_size, feature_size, intermediate_size) :\n",
    "        super(ConvNet, self).__init__()\n",
    "        conv_net = [ConvLayer(seq_size, feature_size, intermediate_size) for i in range(layer_size)]\n",
    "        self.conv_net = nn.Sequential(*conv_net)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) :\n",
    "        for p in self.parameters() :\n",
    "            if p.requires_grad == True and p.dim() > 1:\n",
    "                nn.init.kaiming_uniform_(p)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        y = self.conv_net(x)\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "sds_conv_layer = ConvLayer(seq_size=384, \n",
    "    feature_size=1024,\n",
    "    intermediate_size=512)\n",
    "\n",
    "cnn_layer_output = sds_conv_layer(model_output)\n",
    "print(cnn_layer_output.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 384, 1024])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "sds_conv_net = ConvNet(layer_size=3,\n",
    "    seq_size=384, \n",
    "    feature_size=1024,\n",
    "    intermediate_size=512)\n",
    "\n",
    "cnn_output = sds_conv_net(model_output)\n",
    "print(cnn_output.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 384, 1024])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM Head"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "model_output.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "class LSTMHead(nn.Module) :\n",
    "    def __init__(self, layer_size, feature_size, intermediate_size) :\n",
    "        super(LSTMHead, self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.feature_size = feature_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size=feature_size,\n",
    "            hidden_size = intermediate_size,\n",
    "            num_layers=layer_size,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) :\n",
    "        for p in self.parameters() :\n",
    "            if p.requires_grad == True and p.dim() > 1:\n",
    "                nn.init.kaiming_uniform_(p)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        print(x.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        h_input = torch.zeros((2*self.layer_size, batch_size, self.intermediate_size))\n",
    "        c_input = torch.zeros((2*self.layer_size, batch_size, self.intermediate_size))\n",
    "        y, (h_output, c_output) = self.lstm(x, (h_input,c_input))\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "lstm_head = LSTMHead(3, 1024, 512)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "lstm_output = lstm_head(model_output)\n",
    "lstm_output.shape"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 384, 1024])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from transformers import BertPreTrainedModel, BertModel, RobertaModel, RobertaPreTrainedModel\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "class SDSNetForQuestionAnswering(RobertaPreTrainedModel):\n",
    "    def __init__(self, model_name, data_args, config):\n",
    "        super(SDSNetForQuestionAnswering, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = RobertaModel.from_pretrained(model_name, \n",
    "            config=config, \n",
    "            add_pooling_layer=False\n",
    "        )\n",
    "\n",
    "        self.cnn_head = ConvNet(layer_size=data_args.cnn_layer_size, \n",
    "            seq_size=data_args.max_seq_length,\n",
    "            feature_size=config.hidden_size,\n",
    "            intermediate_size=data_args.cnn_intermediate_size)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0] # (batch_size, seq_size, hidden_size) : [CLS] Token\n",
    "        sequence_output = self.cnn_head(sequence_output)\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output) # (batch_size, seq_size, label_size=2)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  \n",
    "        start_logits = start_logits.squeeze(-1).contiguous() # (batch_size, seq_size) \n",
    "        end_logits = end_logits.squeeze(-1).contiguous() # (batch_size, seq_size)\n",
    "\n",
    "        total_loss = None\n",
    "        # start_positions : (batch_size, )\n",
    "        # end_positions : (batch_size, )\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            # make answer token logits bigger, find answer position\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index) \n",
    "            start_loss = loss_fct(start_logits, start_positions) \n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "sds_qa = SDSNetForQuestionAnswering(model_name=model_args.model_name_or_path,\n",
    "    data_args=data_args,\n",
    "    config=config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "sds_qa_output = sds_qa(**model_input)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "print('Start Logit Shape : {}'.format(sds_qa_output['start_logits'].shape))\n",
    "print('End Logit Shape : {}'.format(sds_qa_output['end_logits'].shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Logit Shape : torch.Size([1, 384])\n",
      "End Logit Shape : torch.Size([1, 384])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}